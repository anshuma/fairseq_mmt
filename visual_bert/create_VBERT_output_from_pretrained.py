# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18NKKF5aQ4tfrfwK1YiYRDslZaPtkjaqJ
"""

# %pip install-r requirements.txt

import torch
from transformers import VisualBertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments,VisualBertForPreTraining
from transformers import AutoImageProcessor, DeiTModel
from datasets import Dataset, Features, Value, Array3D
from PIL import Image
from torchvision.transforms import ToTensor
import pandas as pd
import os

#from IPython.display import Image
from processing_image import Preprocess
from modeling_frcnn import GeneralizedRCNN
from utils import Config


# load models and model components
frcnn_cfg = Config.from_pretrained("unc-nlp/frcnn-vg-finetuned")

frcnn = GeneralizedRCNN.from_pretrained("unc-nlp/frcnn-vg-finetuned", config=frcnn_cfg)

image_preprocess = Preprocess(frcnn_cfg)

# Define paths
data_dir = '/Users/anshumashuk/git/MTech_IITHyd/IITHyd_Capstone/final_Capstone_experiments/fairseq_mmt/small_dataset/data/multi30k-en-de'
image_dir = '/Users/anshumashuk/git/MTech_IITHyd/IITHyd_Capstone/final_Capstone_experiments/fairseq_mmt/small_dataset/flickr30k/flickr30k-images/'
image_idx_dir = '/Users/anshumashuk/git/MTech_IITHyd/IITHyd_Capstone/final_Capstone_experiments/fairseq_mmt/small_dataset/flickr30k/'
count1 = 0
output_dir = '/Users/anshumashuk/git/MTech_IITHyd/IITHyd_Capstone/final_Capstone_experiments/fairseq_mmt/small_dataset/data/VisualBert_small'

# Ensure output directory exists
os.makedirs(output_dir, exist_ok=True)
# Load indices and captions
def load_data(split):
    try:
        with open(f'{image_idx_dir}/{split}.txt', 'r') as f:
            indices = f.read().splitlines()
        with open(f'{data_dir}/{split}.en', 'r') as f:
            captions = f.read().splitlines()
        data = pd.DataFrame({'index': indices, 'caption': captions})
        return data
    except Exception as e:
        print(f"Error loading {split} data: {e}")
        return None


image_processor = AutoImageProcessor.from_pretrained("facebook/deit-base-distilled-patch16-224")
image_model = DeiTModel.from_pretrained("facebook/deit-base-distilled-patch16-224")
def get_visual_embeddings(image):
    inputs = image_processor(image, return_tensors="pt")

    with torch.no_grad():
        outputs = image_model(**inputs)

    last_hidden_states = outputs.last_hidden_state
    list(last_hidden_states.shape)
    return last_hidden_states

def get_visual_embeddings1(image):
    images, sizes, scales_yx = image_preprocess(image)
    #print('images.shape', images.shape)
    output_dict = frcnn(
        images,
        sizes,
        scales_yx=scales_yx,
        padding="max_detections",
        max_detections=frcnn_cfg.max_detections,
        return_tensors="pt",
    )

    # Very important that the boxes are normalized
    # normalized_boxes = output_dict.get("normalized_boxes")
    features = output_dict.get("roi_features")
    return features

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = VisualBertForPreTraining.from_pretrained('./finetuned_model_VisualBERT')
# Preprocess function

def preprocess_example(image_path, text):
    image_path = os.path.join(image_path)
    #image = Image.open(image_path).convert('RGB')
    #visual_embeds = get_visual_embeddings(image).unsqueeze(0)
    visual_embeds = get_visual_embeddings1([image_path])
    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)

    inputs = tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')
    max_length = inputs["input_ids"].shape[-1] + visual_embeds.shape[-2]
    #print('max_length',max_length)
    labels = tokenizer(text, return_tensors="pt", padding="max_length", max_length=max_length)[
        "input_ids"]
    #print('labels.shape',labels.shape)
    sentence_image_labels = torch.tensor(1).unsqueeze(0)
    inputs.update({
        "visual_embeds": visual_embeds,
        "visual_token_type_ids": visual_token_type_ids,
        "visual_attention_mask": visual_attention_mask,
        "labels":labels.squeeze(0),
        "sentence_image_labels":sentence_image_labels
    })
    global count1
    if(count1 == 0):
        print('visual_embeds',visual_embeds.shape)
        print('visual_attention_mask.shape',visual_attention_mask.shape)
        print('attention_mask.shape', inputs['attention_mask'].shape)
    count1 = count1+1
    print('count', count1)
    if(count1 % 50 == 0):
        print('image_path',image_path)
        print('count1',count1)
    return inputs
def load_and_preprocess_data(split):
    data = load_data(split)
    encodings = []
    if data is not None:
        for index, row in data.iterrows():
            inputs = preprocess_example(os.path.join(image_dir, f"{row['index']}"), row['caption'])
            encodings.append(inputs)
    return encodings

# Load indices and captions
# def load_data(split):
#     try:
#         with open(f'{image_idx_dir}/{split}.txt', 'r') as f:
#             indices = f.read().splitlines()
#         with open(f'{data_dir}/{split}.en', 'r') as f:
#             captions = f.read().splitlines()
#         data = pd.DataFrame({'index': indices, 'caption': captions})
#         return data
#     except Exception as e:
#         print(f"Error loading {split} data: {e}")
#         return None

# Generat

count = 1
def generate_and_save_predictions(encodings, split):
    tmp = []
    tmp1 = []
    global count
    model.eval()
    predictions = []
    for inputs in encodings:
        with torch.no_grad():
            outputs = model(**inputs, output_hidden_states=True)
            prediction_logits = outputs.prediction_logits
            predicted_tokens = torch.argmax(prediction_logits, dim=-1)
            predicted_sentence = tokenizer.decode(predicted_tokens[0], skip_special_tokens=True)
            predictions.append(predicted_sentence)
            #last_layer_output = outputs.hidden_states[-1]
            #last_layer_output_prediction = prediction_logits
            last_layer_output = outputs.hidden_states[-1]
            print('last_layer_output', last_layer_output.shape)
            #print('last_layer_output_prediction', last_layer_output_prediction.shape)
            #torch.save(last_layer_output, os.path.join(output_dir, f'final_{split}_{i}.pth'))
            tmp.append(last_layer_output.detach())
            #tmp1.append(last_layer_output_prediction.detach())
            if len(tmp) == 2000:
                res = torch.cat(tmp)
                print(res.shape)
                #res1 = torch.cat(tmp1)
                #print(res1.shape)
                torch.save(res, os.path.join(output_dir, str(count) + split + '.pth'))
                #torch.save(res1, os.path.join(output_dir, str(count) + split + '_predict'+'.pth'))
                count += 1
                tmp = []
                #tmp1 = []

    with open(os.path.join(output_dir, f"{split}_predictions.txt"), 'w') as f:
        for prediction in predictions:
            f.write(f"{prediction}\n")

    print('tmp', tmp)
    res = torch.cat(tmp)
    #res1 = torch.cat(tmp1)
    if count > 1:
        torch.save(res, os.path.join(output_dir, 'final' + split + '.pth'))
        #torch.save(res1, os.path.join(output_dir, 'final' + split + '_predict'+'.pth'))
    else:
        print('feature shape:', res.shape, ',save in:', output_dir + '/' + split + '.pth')
        torch.save(res, os.path.join(output_dir, split + '.pth'))
        #torch.save(res1, os.path.join(output_dir, split + '_predict' + '.pth'))
    del tmp
    #del tmp1
    _tmp = []
    #_tmp1 = []
    if count > 1:
        for i in range(1, count):
            _tmp.append(torch.load(os.path.join(output_dir, str(i) + split + '.pth')))
            #_tmp1.append(torch.load(os.path.join(output_dir, str(i) + split + '_predict'+'.pth')))
        _tmp.append(torch.load(os.path.join(output_dir, 'final' + split + '.pth')))
        #_tmp1.append(torch.load(os.path.join(output_dir, 'final' + split + '_predict' + '.pth')))
        res = torch.cat(_tmp).cpu()
        #res1 = torch.cat(_tmp1).cpu()
        print('feature shape:', res.shape, ',save in:', output_dir + '/' + split + '.pth')
        torch.save(res, os.path.join(output_dir, split + '.pth'))
        #torch.save(res1, os.path.join(output_dir, split + '_predict'+'.pth'))

        # delete
        for i in range(1, count):
            os.remove(os.path.join(output_dir, str(i) + split + '.pth'))
            #os.remove(os.path.join(output_dir, str(i) + split + '_predict'+'.pth'))
        os.remove(os.path.join(output_dir, 'final' + split + '.pth'))
        #os.remove(os.path.join(output_dir, 'final' + split + '_predict'+'.pth'))

# Process and generate outputs for train, validation, and test sets
#train_encodings = load_and_preprocess_data('small_train')
valid_encodings = load_and_preprocess_data('valid')
#test_encodings = load_and_preprocess_data('test.2016')

#generate_and_save_predictions(train_encodings, 'small_train')
generate_and_save_predictions(valid_encodings, 'valid')
#generate_and_save_predictions(test_encodings, 'test.2016')