# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18NKKF5aQ4tfrfwK1YiYRDslZaPtkjaqJ
"""

# %pip install-r requirements.txt

import torch
from transformers import  BlipProcessor, BlipForConditionalGeneration, BertTokenizer, Trainer, TrainingArguments,VisualBertForPreTraining
from datasets import Dataset, Features, Value, Array3D
from PIL import Image

import pandas as pd
import os




# load models and model components
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load models and model components
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")#.to(device)
blip_model.eval()

# Define paths
data_dir = '../small_dataset/data/multi30k-en-de'
image_dir = '../small_dataset/flickr30k/flickr30k-images/'
image_idx_dir = '../small_dataset/flickr30k/'
count = 0
# Load indices and captions
def load_data(split):
    try:
        with open(f'{image_idx_dir}/{split}.txt', 'r') as f:
            indices = f.read().splitlines()
        with open(f'{data_dir}/{split}.de', 'r') as f:
            captions = f.read().splitlines()
        data = pd.DataFrame({'index': indices, 'caption': captions})
        return data
    except Exception as e:
        print(f"Error loading {split} data: {e}")
        return None

linear_projection = torch.nn.Linear(1024, 2048)#.to(device)


def get_visual_embedding_blip(image_path):
    img = Image.open(image_path).convert("RGB")
    inputs = blip_processor(images=img, return_tensors="pt")#.to(device)
    vision_outputs = blip_model.vision_model(pixel_values=inputs['pixel_values'], return_dict=True)
    out = vision_outputs.last_hidden_state
    out = linear_projection(out)
    return out

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
# Preprocess function
def preprocess_data(row):
    image_path = os.path.join(image_dir, f"{row['index']}")
    #image = Image.open(image_path).convert('RGB')
    #visual_embeds = get_visual_embeddings(image).unsqueeze(0)
    visual_embeds = get_visual_embedding_blip(image_path)
    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)

    inputs = tokenizer(row['caption'], padding='max_length', truncation=True, return_tensors='pt')
    max_length = inputs["input_ids"].shape[-1] + visual_embeds.shape[-2]
    #print('max_length',max_length)
    labels = tokenizer(row['caption'], return_tensors="pt", padding="max_length", max_length=max_length)[
        "input_ids"].float()
    #print('labels.shape',labels.shape)
    sentence_image_labels = torch.tensor(1).unsqueeze(0)
    inputs.update({
        "visual_embeds": visual_embeds,
        "visual_token_type_ids": visual_token_type_ids,
        "visual_attention_mask": visual_attention_mask,
        "labels":labels.squeeze(0),
        "sentence_image_labels":sentence_image_labels
    })
    global count
    if(count == 0):
        print('visual_embeds',visual_embeds.shape)
        print('visual_attention_mask.shape',visual_attention_mask.shape)
        print('attention_mask.shape', inputs['attention_mask'].shape)
    count = count+1
    print('count', count)
    if(count % 50 == 0):
        print('image_path',image_path)
        print('count',count)
    return {key: value.squeeze(0) for key, value in inputs.items()}



# Load and preprocess datasets
train_data = load_data('train')
valid_data = load_data('valid')
test_data = load_data('test.2016')

if train_data is not None:
    train_encodings = train_data.apply(preprocess_data, axis=1).tolist()
if valid_data is not None:
    valid_encodings = valid_data.apply(preprocess_data, axis=1).tolist()
if test_data is not None:
    test_encodings = test_data.apply(preprocess_data, axis=1).tolist()

# Convert to Hugging Face Dataset
features = Features({
    'input_ids': Value(dtype='int32'),
    'attention_mask': Value(dtype='int32'),
    'token_type_ids': Value(dtype='int32'),
    'visual_embeds': Array3D(dtype='float32', shape=(198, 2048)),
    'visual_token_type_ids': Value(dtype='int32'),
    'visual_attention_mask': Value(dtype='float32'),
    'labels': Value(dtype='float32')  # Ensure labels are float
})


def create_dataset(encodings):
    dict_data = {key: [d[key].tolist() for d in encodings] for key in encodings[0]}
    return Dataset.from_dict(dict_data)


train_dataset = create_dataset(train_encodings) if train_data is not None else None
valid_dataset = create_dataset(valid_encodings) if valid_data is not None else None
test_dataset = create_dataset(test_encodings) if test_data is not None else None

# Initialize the model
model = VisualBertForPreTraining.from_pretrained("uclanlp/visualbert-vqa-coco-pre")

print('started training')
# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
)

# Fine-tune the model
if train_dataset is not None:
    trainer.train()

trainer.save_model("./finetuned_model_VisualBERT_large_DE")
tokenizer.save_pretrained("./finetuned_model_VisualBERT_large_DE")

# Evaluate the model
#if test_dataset is not None:
#    trainer.evaluate(eval_dataset=test_dataset)
