# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18NKKF5aQ4tfrfwK1YiYRDslZaPtkjaqJ
"""

# %pip install-r requirements.txt

import torch
from transformers import VisualBertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments,VisualBertForPreTraining
from transformers import AutoImageProcessor, DeiTModel
from datasets import Dataset, Features, Value, Array3D
from PIL import Image
from torchvision.transforms import ToTensor
import pandas as pd
import os

#from IPython.display import Image
from processing_image import Preprocess
from modeling_frcnn import GeneralizedRCNN
from utils import Config
import sys


# load models and model components
frcnn_cfg = Config.from_pretrained("unc-nlp/frcnn-vg-finetuned")

frcnn = GeneralizedRCNN.from_pretrained("unc-nlp/frcnn-vg-finetuned", config=frcnn_cfg)

image_preprocess = Preprocess(frcnn_cfg)

# Define paths
data_dir = '../data/multi30k-en-de'
image_dir = '../flickr30k/flickr30k-images/'
image_idx_dir = '../flickr30k/'
count = 0
# Load indices and captions
def load_data(split):
    try:
        with open(f'{image_idx_dir}/{split}.txt', 'r') as f:
            indices = f.read().splitlines()
        with open(f'{data_dir}/{split}.en', 'r') as f:
            captions = f.read().splitlines()
        data = pd.DataFrame({'index': indices, 'caption': captions})
        return data
    except Exception as e:
        print(f"Error loading {split} data: {e}")
        return None


image_processor = AutoImageProcessor.from_pretrained("facebook/deit-base-distilled-patch16-224")
image_model = DeiTModel.from_pretrained("facebook/deit-base-distilled-patch16-224")
def get_visual_embeddings(image):
    inputs = image_processor(image, return_tensors="pt")

    with torch.no_grad():
        outputs = image_model(**inputs)

    last_hidden_states = outputs.last_hidden_state
    list(last_hidden_states.shape)
    return last_hidden_states

def get_visual_embeddings1(image):
    images, sizes, scales_yx = image_preprocess(image)
    #print('images.shape', images.shape)
    output_dict = frcnn(
        images,
        sizes,
        scales_yx=scales_yx,
        padding="max_detections",
        max_detections=frcnn_cfg.max_detections,
        return_tensors="pt",
    )

    # Very important that the boxes are normalized
    # normalized_boxes = output_dict.get("normalized_boxes")
    features = output_dict.get("roi_features")
    return features

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
# Preprocess function
def preprocess_data(row):
    image_path = os.path.join(image_dir, f"{row['index']}")
    #image = Image.open(image_path).convert('RGB')
    #visual_embeds = get_visual_embeddings(image).unsqueeze(0)
    visual_embeds = get_visual_embeddings1([image_path])
    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)
    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)

    inputs = tokenizer(row['caption'], padding='max_length', truncation=True, return_tensors='pt')
    max_length = inputs["input_ids"].shape[-1] + visual_embeds.shape[-2]
    #print('max_length',max_length)
    labels = tokenizer(row['caption'], return_tensors="pt", padding="max_length", max_length=max_length)[
        "input_ids"].float()
    #print('labels.shape',labels.shape)
    sentence_image_labels = torch.tensor(1).unsqueeze(0)
    inputs.update({
        "visual_embeds": visual_embeds,
        "visual_token_type_ids": visual_token_type_ids,
        "visual_attention_mask": visual_attention_mask,
        "labels":labels.squeeze(0),
        "sentence_image_labels":sentence_image_labels
    })
    global count
    if(count == 0):
        print('visual_embeds',visual_embeds.shape)
        print('visual_attention_mask.shape',visual_attention_mask.shape)
        print('attention_mask.shape', inputs['attention_mask'].shape)
    count = count+1
    print('count', count)
    if(count % 50 == 0):
        print('image_path',image_path)
        print('count',count)
    return {key: value.squeeze(0) for key, value in inputs.items()}



# Load and preprocess datasets
train_data = load_data('train')
valid_data = load_data('valid')
test_data = load_data('test.2016')

if train_data is not None:
    train_encodings = train_data.apply(preprocess_data, axis=1).tolist()
if valid_data is not None:
    valid_encodings = valid_data.apply(preprocess_data, axis=1).tolist()
if test_data is not None:
    test_encodings = test_data.apply(preprocess_data, axis=1).tolist()

# Convert to Hugging Face Dataset
features = Features({
    'input_ids': Value(dtype='int32'),
    'attention_mask': Value(dtype='int32'),
    'token_type_ids': Value(dtype='int32'),
    'visual_embeds': Array3D(dtype='float32', shape=(198, 2048)),
    'visual_token_type_ids': Value(dtype='int32'),
    'visual_attention_mask': Value(dtype='float32'),
    'labels': Value(dtype='float32')  # Ensure labels are float
})


def create_dataset(encodings):
    dict_data = {key: [d[key].tolist() for d in encodings] for key in encodings[0]}
    return Dataset.from_dict(dict_data)


train_dataset = create_dataset(train_encodings) if train_data is not None else None
valid_dataset = create_dataset(valid_encodings) if valid_data is not None else None
test_dataset = create_dataset(test_encodings) if test_data is not None else None

# Initialize the model
model = VisualBertForPreTraining.from_pretrained("uclanlp/visualbert-vqa-coco-pre")

print('started training')
# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
)

# Fine-tune the model
if train_dataset is not None:
    trainer.train()

trainer.save_model("./finetuned_model_VisualBERT_large")
tokenizer.save_pretrained("./finetuned_model_VisualBERT_large")

# Evaluate the model
if test_dataset is not None:
    trainer.evaluate(eval_dataset=test_dataset)

