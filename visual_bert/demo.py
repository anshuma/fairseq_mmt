# -*- coding: utf-8 -*-
"""demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18NKKF5aQ4tfrfwK1YiYRDslZaPtkjaqJ
"""

# %pip install-r requirements.txt

import torch
from transformers import VisualBertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments,VisualBertForPreTraining,BlipProcessor, BlipForConditionalGeneration
from transformers import AutoImageProcessor, DeiTModel
from datasets import Dataset, Features, Value, Array3D
from PIL import Image
from torchvision.transforms import ToTensor
import pandas as pd
import os

#from IPython.display import Image
from processing_image import Preprocess
from modeling_frcnn import GeneralizedRCNN
from utils import Config

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#device =("cpu")
# load models and model components
frcnn_cfg = Config.from_pretrained("unc-nlp/frcnn-vg-finetuned")

frcnn = GeneralizedRCNN.from_pretrained("unc-nlp/frcnn-vg-finetuned", config=frcnn_cfg).to(device)

image_preprocess = Preprocess(frcnn_cfg)

# Define paths
data_dir = '../data/multi30k-en-de'
image_dir = '../flickr30k/flickr30k-images/'
image_idx_dir = '../flickr30k/'
count = 0
# Load indices and captions
def load_data(split):
    try:
        with open(f'{image_idx_dir}/{split}.txt', 'r') as f:
            indices = f.read().splitlines()
        with open(f'{data_dir}/{split}.de', 'r') as f:
            captions = f.read().splitlines()
        with open(f'{data_dir}/{split}_T5.de', 'r') as f:
            captions1 = f.read().splitlines()
        combined_captions = [f"{cap} {cap1}" for cap, cap1 in zip(captions, captions1)]
        print("combined_captions[0]",combined_captions[0])
        data = pd.DataFrame({'index': indices, 'caption': combined_captions})
        #data = pd.DataFrame({'index': indices, 'caption': captions1})
        return data
    except Exception as e:
        print(f"Error loading {split} data: {e}")
        return None


image_processor = AutoImageProcessor.from_pretrained("facebook/deit-base-distilled-patch16-224")
image_model = DeiTModel.from_pretrained("facebook/deit-base-distilled-patch16-224")
def get_visual_embeddings(image):
    inputs = image_processor(image, return_tensors="pt")

    with torch.no_grad():
        outputs = image_model(**inputs)

    last_hidden_states = outputs.last_hidden_state
    list(last_hidden_states.shape)
    return last_hidden_states

def get_visual_embeddings1(image):
    images, sizes, scales_yx = image_preprocess(image)
    images = images.to(device)
    sizes = sizes.to(device)
    scales_yx = scales_yx.to(device)
    #print('images.shape', images.shape)
    output_dict = frcnn(
        images,
        sizes,
        scales_yx=scales_yx,
        padding="max_detections",
        max_detections=frcnn_cfg.max_detections,
        return_tensors="pt",
    )

    # Very important that the boxes are normalized
    # normalized_boxes = output_dict.get("normalized_boxes")
    features = output_dict.get("roi_features").to(device)
    return features

torch.cuda.empty_cache()
# Define device as CUDA if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load models and model components
blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large").to(device)
blip_model.eval()

linear_projection = torch.nn.Linear(1024, 2048).to(device)


def get_visual_embedding_blip(image_path):
    img = Image.open(image_path).convert("RGB")
    inputs = blip_processor(images=img, return_tensors="pt").to(device)
    vision_outputs = blip_model.vision_model(pixel_values=inputs['pixel_values'], return_dict=True)
    out = vision_outputs.last_hidden_state
    print('out.shape',out.shape)
    out = linear_projection(out)
    return out

# Initialize tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
# Preprocess function
def preprocess_data(row):
    image_path = os.path.join(image_dir, f"{row['index']}")
    #image = Image.open(image_path).convert('RGB')
    #visual_embeds = get_visual_embeddings(image).unsqueeze(0)
    visual_embeds = get_visual_embeddings1([image_path])
    #visual_embeds = get_visual_embedding_blip(image_path)
    visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long , device=device)
    visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float, device=device)

    inputs = tokenizer(row['caption'], padding='max_length', truncation=True, return_tensors='pt').to(device)
    max_length = inputs["input_ids"].shape[-1] + visual_embeds.shape[-2]
    #print('max_length',max_length)
    labels = tokenizer(row['caption'], return_tensors="pt", padding="max_length", max_length=max_length)[
        "input_ids"].to(device)
    #print('labels.shape',labels.shape)
    sentence_image_labels = torch.tensor(1).unsqueeze(0).to(device)
    inputs.update({
        "visual_embeds": visual_embeds,
        "visual_token_type_ids": visual_token_type_ids,
        "visual_attention_mask": visual_attention_mask,
        "labels":labels.squeeze(0),
        "sentence_image_labels":sentence_image_labels
    })
    global count
    if(count == 0):
        print('visual_embeds',visual_embeds.shape)
        print('visual_attention_mask.shape',visual_attention_mask.shape)
        print('attention_mask.shape', inputs['attention_mask'].shape)
    count = count+1
    print('count', count)
    if(count % 50 == 0):
        print('image_path',image_path)
        print('count',count)
    return {key: value.squeeze(0) for key, value in inputs.items()}



# Load and preprocess datasets
train_data = load_data('train')
valid_data = load_data('valid')
#test_data = load_data('test.2016')

if train_data is not None:
    train_encodings = train_data.apply(preprocess_data, axis=1).tolist()
if valid_data is not None:
    valid_encodings = valid_data.apply(preprocess_data, axis=1).tolist()
#if test_data is not None:
#    test_encodings = test_data.apply(preprocess_data, axis=1).tolist()

# Convert to Hugging Face Dataset
features = Features({
    'input_ids': Value(dtype='int32'),
    'attention_mask': Value(dtype='int32'),
    'token_type_ids': Value(dtype='int32'),
    'visual_embeds': Array3D(dtype='float32', shape=(198, 2048)),
    'visual_token_type_ids': Value(dtype='int32'),
    'visual_attention_mask': Value(dtype='float32'),
    'labels': Value(dtype='float32')  # Ensure labels are float
})


def create_dataset(encodings):
    dict_data = {key: [d[key].tolist() for d in encodings] for key in encodings[0]}
    return Dataset.from_dict(dict_data)


train_dataset = create_dataset(train_encodings) if train_data is not None else None
valid_dataset = create_dataset(valid_encodings) if valid_data is not None else None
#test_dataset = create_dataset(test_encodings) if test_data is not None else None

# Initialize the model
model = VisualBertForPreTraining.from_pretrained("uclanlp/visualbert-vqa-coco-pre").to(device)

print('started training')
# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
)

# Fine-tune the model
if train_dataset is not None:
    trainer.train()

trainer.save_model("./finetuned_model_VisualBERT_FRCNN_DE_DE")
tokenizer.save_pretrained("./finetuned_model_VisualBERT_FRCNN_DE_DE")

# Evaluate the model
#if test_dataset is not None:
#    trainer.evaluate(eval_dataset=test_dataset)
